{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide this example in two stages: Pre-deployment Stage and Production Stage.\n",
    "\n",
    "In the __Pre-deployment Stage__ we will:\n",
    "- train a classifier\n",
    "- calculate the centroids for each topic cluster\n",
    "\n",
    "In the __Production Stage__ we will:\n",
    "- load daily batches of data\n",
    "- vectorize the data\n",
    "- predict the topic for each document\n",
    "- log:\n",
    "    - embeddings distance to the centroids\n",
    "    - tokens list for each document\n",
    "    - predictions and targets\n",
    "\n",
    "In the Production Stage, we will introduce documents in another language (Spanish) to see how the model behaves, and how we can monitor this with WhyLabs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==1.0.2 whylogs==1.1.31 whylabs-client==0.4.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔️ Setting the Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# # set your org-id here - should be something like \"org-xxxx\"\n",
    "# print(\"Enter your WhyLabs Org ID\") \n",
    "# os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input()\n",
    "\n",
    "# # set your datased_id (or model_id) here - should be something like \"model-xxxx\"\n",
    "# print(\"Enter your WhyLabs Dataset ID\")\n",
    "# os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = input()\n",
    "\n",
    "\n",
    "# # set your API key here\n",
    "# print(\"Enter your WhyLabs API key\")\n",
    "# os.environ[\"WHYLABS_API_KEY\"] = getpass.getpass()\n",
    "# print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from whylogs.experimental.preprocess.embeddings.selectors import PCACentroidsSelector\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"comp.graphics\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"talk.politics.guns\",\n",
    "    \"misc.forsale\",\n",
    "    \"sci.med\",\n",
    "]\n",
    "\n",
    "twenty_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "    ]\n",
    ")\n",
    "vectors_train = vectorizer.fit_transform(twenty_train.data)\n",
    "\n",
    "vectors_train = vectors_train.toarray()\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(vectors_train, twenty_train.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atheism', 'graphics', 'forsale', 'baseball', 'med', 'christian', 'guns']\n"
     ]
    }
   ],
   "source": [
    "references, labels = PCACentroidsSelector(n_components=20).calculate_references(vectors_train, twenty_train.target)\n",
    "ref_labels = [twenty_train.target_names[x].split(\".\")[-1] for x in labels]\n",
    "print(ref_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Schema for Embeddings+Tokens+Performance logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whylogs as why\n",
    "from whylogs.core.resolvers import MetricSpec, ResolverSpec\n",
    "from whylogs.core.schema import DeclarativeSchema\n",
    "from whylogs.experimental.extras.embedding_metric import (\n",
    "    DistanceFunction,\n",
    "    EmbeddingConfig,\n",
    "    EmbeddingMetric,\n",
    ")\n",
    "from whylogs.experimental.extras.nlp_metric import BagOfWordsMetric\n",
    "from whylogs.core.resolvers import STANDARD_RESOLVER\n",
    "\n",
    "\n",
    "config = EmbeddingConfig(\n",
    "    references=references,\n",
    "    labels=ref_labels,\n",
    "    distance_fn=DistanceFunction.cosine,\n",
    ")\n",
    "embeddings_resolver = ResolverSpec(column_name=\"news_centroids\", metrics=[MetricSpec(EmbeddingMetric, config)])\n",
    "tokens_resolver = ResolverSpec(column_name=\"document_tokens\", metrics=[MetricSpec(BagOfWordsMetric)])\n",
    "\n",
    "schema = DeclarativeSchema(STANDARD_RESOLVER+[embeddings_resolver,tokens_resolver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading daily batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tokens</th>\n",
       "      <th>language</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello\\n\\n        Just one quick question\\n    ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>['Hello', 'Just', 'one', 'quick', 'question', ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OFFICIAL UNITED NATIONS SOUVENIR FOLDERS\\n\\nEa...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['OFFICIAL', 'UNITED', 'NATIONS', 'SOUVENIR', ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am selling Joe Montana SportsTalk Football 9...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['I', 'selling', 'Joe', 'Montana', 'SportsTalk...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nNonsteroid  Proventil is a brand of albute...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>['Nonsteroid', 'Proventil', 'brand', 'albutero...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two URGENT requests\\n\\n1 I need the latest upd...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>['Two', 'URGENT', 'requests', '1', 'I', 'need'...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  target  predicted  \\\n",
       "0  Hello\\n\\n        Just one quick question\\n    ...       4          4   \n",
       "1  OFFICIAL UNITED NATIONS SOUVENIR FOLDERS\\n\\nEa...       2          2   \n",
       "2  I am selling Joe Montana SportsTalk Football 9...       2          2   \n",
       "3  \\n\\nNonsteroid  Proventil is a brand of albute...       4          4   \n",
       "4  Two URGENT requests\\n\\n1 I need the latest upd...       6          6   \n",
       "\n",
       "                                              tokens language  batch_id  \\\n",
       "0  ['Hello', 'Just', 'one', 'quick', 'question', ...       en         0   \n",
       "1  ['OFFICIAL', 'UNITED', 'NATIONS', 'SOUVENIR', ...       en         0   \n",
       "2  ['I', 'selling', 'Joe', 'Montana', 'SportsTalk...       en         0   \n",
       "3  ['Nonsteroid', 'Proventil', 'brand', 'albutero...       en         0   \n",
       "4  ['Two', 'URGENT', 'requests', '1', 'I', 'need'...       en         0   \n",
       "\n",
       "   doc_id  \n",
       "0     0.0  \n",
       "1     1.0  \n",
       "2     2.0  \n",
       "3     3.0  \n",
       "4     4.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_df = pd.read_csv(\"prod_df_20ng.csv\")\n",
    "prod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Perturbation - Spanish Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_perturbation_ratio = [0,0,0,0,0.33,0.66,1]\n",
    "\n",
    "def get_docs_by_language_ratio(batch_df, ratio):\n",
    "    n_docs = len(batch_df[batch_df[\"language\"] == \"en\"])\n",
    "    n_es_docs = int(n_docs * ratio)\n",
    "    n_en_docs = n_docs - n_es_docs\n",
    "    en_df = batch_df[batch_df[\"language\"] == \"en\"].sample(n_en_docs)    \n",
    "    es_df = batch_df[~batch_df['doc_id'].isin(en_df[\"doc_id\"])]\n",
    "    # filter out docs with doc_id in en_df\n",
    "\n",
    "\n",
    "    es_df = es_df[es_df[\"language\"] == \"es\"].sample(n_es_docs)\n",
    "    docs = pd.concat([en_df, es_df])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log and Upload to WhyLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day 0: 2023-03-30 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8509485094850948\n",
      "day 1: 2023-03-29 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8310991957104558\n",
      "day 2: 2023-03-28 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8306451612903226\n",
      "day 3: 2023-03-27 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8118279569892473\n",
      "day 4: 2023-03-26 00:00:00+00:00\n",
      "33.0% of documents with language perturbation\n",
      "mean accuracy:  0.6380697050938338\n",
      "day 5: 2023-03-25 00:00:00+00:00\n",
      "66.0% of documents with language perturbation\n",
      "mean accuracy:  0.4702702702702703\n",
      "day 6: 2023-03-24 00:00:00+00:00\n",
      "100% of documents with language perturbation\n",
      "mean accuracy:  0.2923497267759563\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta, timezone\n",
    "import whylogs as why\n",
    "\n",
    "\n",
    "\n",
    "for day, batch_df in prod_df.groupby(\"batch_id\"):\n",
    "    dataset_timestamp = datetime.now() - timedelta(days=day)\n",
    "    dataset_timestamp = dataset_timestamp.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo = timezone.utc)\n",
    "\n",
    "    print(f\"day {day}: {dataset_timestamp}\")\n",
    "\n",
    "    ratio = language_perturbation_ratio[day]\n",
    "    print(f\"{ratio*100}% of documents with language perturbation\")\n",
    " \n",
    "    mixed_df = get_docs_by_language_ratio(batch_df, ratio)\n",
    "    mixed_df = mixed_df.dropna()\n",
    "\n",
    "    vectors = vectorizer.transform(mixed_df['doc']).toarray()\n",
    "    predicted = clf.predict(vectors)\n",
    "    print(\"mean accuracy: \", np.mean(predicted == mixed_df['target']))\n",
    "\n",
    "\n",
    "    profile = why.log(row={\"news_centroids\": vectors,\n",
    "                                     \"document_tokens\": mixed_df[\"tokens\"]},\n",
    "                                     schema=schema)\n",
    "    profile.set_dataset_timestamp(dataset_timestamp)\n",
    "\n",
    "    output_df = pd.DataFrame({\"output_target\": mixed_df[\"target\"],\n",
    "                            \"output_prediction\": predicted})\n",
    "\n",
    "    results = why.log_classification_metrics(\n",
    "        output_df,\n",
    "        target_column=\"output_target\",\n",
    "        prediction_column=\"output_prediction\",\n",
    "    )\n",
    "    results.set_dataset_timestamp(dataset_timestamp)    \n",
    "\n",
    "    # results.writer(\"whylabs\").write()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
