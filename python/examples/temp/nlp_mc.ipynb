{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Text Embeddings with the 20 Newsgroups Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide this example in two stages: Pre-deployment Stage and Production Stage.\n",
    "\n",
    "In the __Pre-deployment Stage__ we will:\n",
    "- train a classifier\n",
    "- calculate the centroids for each topic cluster\n",
    "\n",
    "In the __Production Stage__ we will:\n",
    "- load daily batches of data\n",
    "- vectorize the data\n",
    "- predict the topic for each document\n",
    "- log:\n",
    "    - embeddings distance to the centroids\n",
    "    - tokens list for each document\n",
    "    - predictions and targets\n",
    "\n",
    "In the Production Stage, we will introduce documents in another language (Spanish) to see how the model behaves, and how we can monitor this with WhyLabs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install whylogs[whylabs] scikit-learn==1.0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔️ Setting the Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# # set your org-id here - should be something like \"org-xxxx\"\n",
    "# print(\"Enter your WhyLabs Org ID\") \n",
    "# os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input()\n",
    "\n",
    "# # set your datased_id (or model_id) here - should be something like \"model-xxxx\"\n",
    "# print(\"Enter your WhyLabs Dataset ID\")\n",
    "# os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = input()\n",
    "\n",
    "\n",
    "# # set your API key here\n",
    "# print(\"Enter your WhyLabs API key\")\n",
    "# os.environ[\"WHYLABS_API_KEY\"] = getpass.getpass()\n",
    "# print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your WhyLabs Org ID\n",
      "Enter your WhyLabs Dataset ID\n",
      "Enter your WhyLabs API key\n",
      "Using API Key ID:  z8fYdnQwHr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WHYLABS_API_ENDPOINT\"] = \"https://songbird.development.whylabsdev.com\"\n",
    "# set your org-id here - should be something like \"org-xxxx\"\n",
    "print(\"Enter your WhyLabs Org ID\") \n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = \"org-HVB9AM\"\n",
    "\n",
    "# set your datased_id (or model_id) here - should be something like \"model-xxxx\"\n",
    "print(\"Enter your WhyLabs Dataset ID\")\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = \"model-51\"\n",
    "\n",
    "\n",
    "# set your API key here\n",
    "print(\"Enter your WhyLabs API key\")\n",
    "os.environ[\"WHYLABS_API_KEY\"] = \"z8fYdnQwHr.ibJaqDpZSsZd9dpo5ILyKlOgwXWPV7LGvtIsyqFUs54MGUsHMNz6q\"\n",
    "print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from whylogs.experimental.preprocess.embeddings.selectors import PCACentroidsSelector\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"comp.graphics\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"talk.politics.guns\",\n",
    "    \"misc.forsale\",\n",
    "    \"sci.med\",\n",
    "]\n",
    "\n",
    "twenty_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "    ]\n",
    ")\n",
    "vectors_train = vectorizer.fit_transform(twenty_train.data)\n",
    "\n",
    "vectors_train = vectors_train.toarray()\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(vectors_train, twenty_train.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atheism', 'graphics', 'forsale', 'baseball', 'med', 'christian', 'guns']\n"
     ]
    }
   ],
   "source": [
    "references, labels = PCACentroidsSelector(n_components=20).calculate_references(vectors_train, twenty_train.target)\n",
    "ref_labels = [twenty_train.target_names[x].split(\".\")[-1] for x in labels]\n",
    "print(ref_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Schema for Embeddings+Tokens+Performance logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whylogs as why\n",
    "from whylogs.core.resolvers import MetricSpec, ResolverSpec\n",
    "from whylogs.core.schema import DeclarativeSchema\n",
    "from whylogs.experimental.extras.embedding_metric import (\n",
    "    DistanceFunction,\n",
    "    EmbeddingConfig,\n",
    "    EmbeddingMetric,\n",
    ")\n",
    "from whylogs.experimental.extras.nlp_metric import BagOfWordsMetric\n",
    "from whylogs.core.resolvers import STANDARD_RESOLVER\n",
    "\n",
    "\n",
    "config = EmbeddingConfig(\n",
    "    references=references,\n",
    "    labels=ref_labels,\n",
    "    distance_fn=DistanceFunction.cosine,\n",
    ")\n",
    "embeddings_resolver = ResolverSpec(column_name=\"news_centroids\", metrics=[MetricSpec(EmbeddingMetric, config)])\n",
    "tokens_resolver = ResolverSpec(column_name=\"document_tokens\", metrics=[MetricSpec(BagOfWordsMetric)])\n",
    "\n",
    "schema = DeclarativeSchema(STANDARD_RESOLVER+[embeddings_resolver,tokens_resolver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading daily batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, let's download the production data from a public S3 bucket. That way, we won't have to translate or tokenize the documents ourselves.\n",
    "\n",
    "The DataFrame below contains 5306 documents - 2653 in English and 2653 in Spanish. The spanish documents were obtained by simply translating the english ones. Documents that have the same `doc_id` refers to the same document in different languages.\n",
    "\n",
    "The tokenization was done using the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tokens</th>\n",
       "      <th>language</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello\\n\\n        Just one quick question\\n    ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[Hello, Just, one, quick, question, My, father...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OFFICIAL UNITED NATIONS SOUVENIR FOLDERS\\n\\nEa...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[OFFICIAL, UNITED, NATIONS, SOUVENIR, FOLDERS,...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am selling Joe Montana SportsTalk Football 9...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[I, selling, Joe, Montana, SportsTalk, Footbal...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nNonsteroid  Proventil is a brand of albute...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[Nonsteroid, Proventil, brand, albuterol, bron...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two URGENT requests\\n\\n1 I need the latest upd...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[Two, URGENT, requests, 1, I, need, latest, up...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  target  predicted  \\\n",
       "0  Hello\\n\\n        Just one quick question\\n    ...       4          4   \n",
       "1  OFFICIAL UNITED NATIONS SOUVENIR FOLDERS\\n\\nEa...       2          2   \n",
       "2  I am selling Joe Montana SportsTalk Football 9...       2          2   \n",
       "3  \\n\\nNonsteroid  Proventil is a brand of albute...       4          4   \n",
       "4  Two URGENT requests\\n\\n1 I need the latest upd...       6          6   \n",
       "\n",
       "                                              tokens language  batch_id  \\\n",
       "0  [Hello, Just, one, quick, question, My, father...       en         0   \n",
       "1  [OFFICIAL, UNITED, NATIONS, SOUVENIR, FOLDERS,...       en         0   \n",
       "2  [I, selling, Joe, Montana, SportsTalk, Footbal...       en         0   \n",
       "3  [Nonsteroid, Proventil, brand, albuterol, bron...       en         0   \n",
       "4  [Two, URGENT, requests, 1, I, need, latest, up...       en         0   \n",
       "\n",
       "   doc_id  \n",
       "0     0.0  \n",
       "1     1.0  \n",
       "2     2.0  \n",
       "3     3.0  \n",
       "4     4.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_url = \"https://whylabs-public.s3.us-west-2.amazonaws.com/whylogs_examples/Newsgroups/production_en_es.parquet\"\n",
    "prod_df = pd.read_parquet(download_url)\n",
    "prod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Perturbation - Spanish Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_perturbation_ratio = [0,0,0,0,0.33,0.66,1]\n",
    "\n",
    "def get_docs_by_language_ratio(batch_df, ratio):\n",
    "    n_docs = len(batch_df[batch_df[\"language\"] == \"en\"])\n",
    "    n_es_docs = int(n_docs * ratio)\n",
    "    n_en_docs = n_docs - n_es_docs\n",
    "    en_df = batch_df[batch_df[\"language\"] == \"en\"].sample(n_en_docs)    \n",
    "    es_df = batch_df[~batch_df['doc_id'].isin(en_df[\"doc_id\"])]\n",
    "    # filter out docs with doc_id in en_df\n",
    "\n",
    "\n",
    "    es_df = es_df[es_df[\"language\"] == \"es\"].sample(n_es_docs)\n",
    "    docs = pd.concat([en_df, es_df])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log and Upload to WhyLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1158/204968923.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmixed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_docs_by_language_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1158/425841597.py\u001b[0m in \u001b[0;36mget_docs_by_language_ratio\u001b[0;34m(batch_df, ratio)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"es\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_es_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/felip/Documents/Projects-WhyLabs/whylogs/python/.venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5363\u001b[0m             )\n\u001b[1;32m   5364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5365\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5366\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "mixed_df = get_docs_by_language_ratio(batch_df, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day 0: 2023-03-25 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8467966573816156\n",
      "Profiling and logging Embeddings and Tokens...\n",
      "Profiling and logging classification metrics...\n",
      "day 1: 2023-03-26 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8333333333333334\n",
      "Profiling and logging Embeddings and Tokens...\n",
      "Profiling and logging classification metrics...\n",
      "day 2: 2023-03-27 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8253521126760563\n",
      "Profiling and logging Embeddings and Tokens...\n",
      "Profiling and logging classification metrics...\n",
      "day 3: 2023-03-28 00:00:00+00:00\n",
      "0% of documents with language perturbation\n",
      "mean accuracy:  0.8065476190476191\n",
      "Profiling and logging Embeddings and Tokens...\n",
      "Profiling and logging classification metrics...\n",
      "day 4: 2023-03-29 00:00:00+00:00\n",
      "33.0% of documents with language perturbation\n",
      "mean accuracy:  0.6225352112676056\n",
      "Profiling and logging Embeddings and Tokens...\n",
      "Profiling and logging classification metrics...\n",
      "day 5: 2023-03-30 00:00:00+00:00\n",
      "66.0% of documents with language perturbation\n",
      "mean accuracy:  0.513677811550152\n",
      "Profiling and logging Embeddings and Tokens...\n",
      "Profiling and logging classification metrics...\n",
      "day 6: 2023-03-31 00:00:00+00:00\n",
      "100% of documents with language perturbation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1158/896929043.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{ratio*100}% of documents with language perturbation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmixed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_docs_by_language_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmixed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1158/425841597.py\u001b[0m in \u001b[0;36mget_docs_by_language_ratio\u001b[0;34m(batch_df, ratio)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"es\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_es_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/felip/Documents/Projects-WhyLabs/whylogs/python/.venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5363\u001b[0m             )\n\u001b[1;32m   5364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5365\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5366\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta, timezone\n",
    "import whylogs as why\n",
    "from whylogs.api.writer.whylabs import WhyLabsWriter\n",
    "import random\n",
    "\n",
    "writer = WhyLabsWriter()\n",
    "\n",
    "for day, batch_df in prod_df.groupby(\"batch_id\"):\n",
    "    dataset_timestamp = datetime.now() - timedelta(days=6-day)\n",
    "    dataset_timestamp = dataset_timestamp.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo = timezone.utc)\n",
    "\n",
    "    print(f\"day {day}: {dataset_timestamp}\")\n",
    "\n",
    "    ratio = language_perturbation_ratio[day]\n",
    "    print(f\"{ratio*100}% of documents with language perturbation\")\n",
    " \n",
    "    mixed_df = get_docs_by_language_ratio(batch_df, ratio)\n",
    "    mixed_df = mixed_df.dropna()\n",
    "\n",
    "    sample_ratio = random.uniform(0.8, 1) # just to have some variability in the total number of daily docs\n",
    "    mixed_df = mixed_df.sample(frac=sample_ratio).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    vectors = vectorizer.transform(mixed_df['doc']).toarray()\n",
    "    predicted = clf.predict(vectors)\n",
    "    print(\"mean accuracy: \", np.mean(predicted == mixed_df['target']))\n",
    "\n",
    "    print(\"Profiling and logging Embeddings and Tokens...\")\n",
    "    profile = why.log(row={\"news_centroids\": vectors,\n",
    "                                     \"document_tokens\": mixed_df[\"tokens\"]},\n",
    "                                     schema=schema)\n",
    "    profile.set_dataset_timestamp(dataset_timestamp)\n",
    "    writer.write(file=profile.view())\n",
    "\n",
    "\n",
    "    newsgroups_df = pd.DataFrame({\"output_target\": mixed_df[\"target\"],\n",
    "                            \"output_prediction\": predicted})\n",
    "    # to map indices to label names\n",
    "    newsgroups_df[\"output_target\"] = newsgroups_df[\"output_target\"].apply(lambda x: ref_labels[x])\n",
    "    newsgroups_df[\"output_prediction\"] = newsgroups_df[\"output_prediction\"].apply(lambda x: ref_labels[x])\n",
    "    newsgroups_df[\"document_tokens\"] = mixed_df[\"tokens\"]\n",
    "\n",
    "    \n",
    "    print(\"Profiling and logging classification metrics...\")\n",
    "    classification_results = why.log_classification_metrics(\n",
    "        newsgroups_df,\n",
    "        target_column=\"output_target\",\n",
    "        prediction_column=\"output_prediction\",\n",
    "        schema=schema,\n",
    "        log_full_data=True\n",
    "    )\n",
    "    classification_results.set_dataset_timestamp(dataset_timestamp)    \n",
    "    writer.write(file=classification_results.view())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd5901cadfd4b29c2aaf95ecd29c0c3b10829ad94dcfe59437dbee391154aea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
