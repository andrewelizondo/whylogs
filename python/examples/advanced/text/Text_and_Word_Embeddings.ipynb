{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
    ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Getting_Started)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Getting_Started) to leverage the power of whylogs and WhyLabs together!*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling text and text embeddings data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/experimental/embeddings/Embeddings_Distance_Logging.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensional embedding spaces can be difficult to understand because we often rely on our own subjective judgement of clusters in the space. Often, data scientists try to find issues solely by hovering over individual data points and noting trends in which ones feel out of place.\n",
    "\n",
    "WhyLabs has a number of features that are useful for natural language processing and text. In this notebook, we'll look at both our unicode tracking features for text and our embeddings features for word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install package extras for whylogs\n",
    "\n",
    "For convenience, we include helper functions to select reference data points for comparing new embedding vectors against. To follow this notebook in full, install the `embeddings` extra (for helper functions) and `viz` extra (for visualizing drift) when installing whylogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade whylogs[embeddings,viz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import whylogs as why\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading word lists\n",
    "\n",
    "We won't train a model for this notebook, but instead look at how our text features work on a stream of data inputs that are organized by topic. Those topics are food, geography, machine learning, and musical instruments.\n",
    "\n",
    "We'd like to use functionality in whylogs to note data quality differences as well as drift when a new category (sports) is added."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading GLoVE vectors\n",
    "\n",
    "We'll use the 200-dimensional GLoVE word embedddings to encode our words in. This can be downloaded from OpenML via scikit-learn. Because the download can take a few minutes, we suggest saving the data locally as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernease/miniconda3/envs/wl-public/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for our example\n",
    "\n",
    "Here, we pull our vocabulary from the different word lists and grabbing the word embedding for those relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dimensions = 200\n",
    "\n",
    "words = []\n",
    "categories = []\n",
    "embeddings = []\n",
    "\n",
    "for list_file in glob.glob(\"wordlists/*\"):\n",
    "    with open(list_file, \"r\") as f:\n",
    "        word_list = f.readlines()\n",
    "        word_list = [word[:-1] for word in word_list]\n",
    "        category_list = [list_file.split(\"/\")[1][:-4]] * len(word_list)\n",
    "        words.extend(word_list)\n",
    "        categories.extend(category_list)\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        embeddings.append(glove_vectors.get_vector(word))\n",
    "    except:\n",
    "        f\"Couldn't find embedding for word {word}. Skipping.\"\n",
    "        embeddings.append(np.zeros(num_dimensions))\n",
    "    if len(embeddings[-1]) != 200:\n",
    "        print(word)\n",
    "\n",
    "X_words = np.array(words)\n",
    "X_embeddings = np.array(embeddings)\n",
    "y = np.array(categories)\n",
    "\n",
    "X_train, X_prod, y_train, y_prod = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and production datasets\n",
    "\n",
    "Instead of training a model, we'll use the same functionality to split our dataset into an original training dataset and data we'll see in our first day of production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_words_train, X_words_prod, X_embed_train, X_embed_prod, y_train, y_prod = train_test_split(X_words, X_embeddings, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hypothesis</td>\n",
       "      <td>[0.3481999933719635, 0.05061199888586998, 0.46...</td>\n",
       "      <td>machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>intercept</td>\n",
       "      <td>[0.4326600134372711, -0.4637100100517273, 0.18...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boundary</td>\n",
       "      <td>[0.05375000089406967, -0.3873499929904938, 0.4...</td>\n",
       "      <td>machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cape</td>\n",
       "      <td>[-0.08508200198411942, -0.1590999960899353, -0...</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>river</td>\n",
       "      <td>[-0.33959999680519104, -0.034967999905347824, ...</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>wheat</td>\n",
       "      <td>[0.3941600024700165, 0.20038999617099762, 0.51...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>pass</td>\n",
       "      <td>[-0.2054699957370758, 0.08040100336074829, -0....</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>cookie</td>\n",
       "      <td>[-0.2373799979686737, 0.504580020904541, 0.011...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>territory</td>\n",
       "      <td>[0.2437400072813034, 0.00025101000210270286, 0...</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>piccolo</td>\n",
       "      <td>[0.3828499913215637, 0.0947050005197525, 0.177...</td>\n",
       "      <td>musicinstruments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words                                         embeddings  \\\n",
       "0    hypothesis  [0.3481999933719635, 0.05061199888586998, 0.46...   \n",
       "1     intercept  [0.4326600134372711, -0.4637100100517273, 0.18...   \n",
       "2      boundary  [0.05375000089406967, -0.3873499929904938, 0.4...   \n",
       "3          cape  [-0.08508200198411942, -0.1590999960899353, -0...   \n",
       "4         river  [-0.33959999680519104, -0.034967999905347824, ...   \n",
       "..          ...                                                ...   \n",
       "266       wheat  [0.3941600024700165, 0.20038999617099762, 0.51...   \n",
       "267        pass  [-0.2054699957370758, 0.08040100336074829, -0....   \n",
       "268      cookie  [-0.2373799979686737, 0.504580020904541, 0.011...   \n",
       "269   territory  [0.2437400072813034, 0.00025101000210270286, 0...   \n",
       "270     piccolo  [0.3828499913215637, 0.0947050005197525, 0.177...   \n",
       "\n",
       "               labels  \n",
       "0     machinelearning  \n",
       "1              sports  \n",
       "2     machinelearning  \n",
       "3           geography  \n",
       "4           geography  \n",
       "..                ...  \n",
       "266              food  \n",
       "267            sports  \n",
       "268              food  \n",
       "269         geography  \n",
       "270  musicinstruments  \n",
       "\n",
       "[271 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.DataFrame({\"words\": X_words_train, \"embeddings\": [val for val in X_embed_train], \"labels\": y_train})\n",
    "df_prod = pd.DataFrame({\"words\": X_words_prod, \"embeddings\": [val for val in X_embed_prod]})\n",
    "\n",
    "display(df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling with whylogs\n",
    "\n",
    "As with other advanced features, we can create a `DeclarativeSchema` to tell whylogs to resolve columns of a certain name to the `EmbeddingMetric` that we want to use.\n",
    "\n",
    "We must pass our references, labels, and preferred distance function (either cosine distance or Euclidean distance) as parameters to `EmbeddingConfig` then log as normal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode and string length metrics for strings\n",
    "\n",
    "By default, columns of type `str` will have the following metrics, when logged with whylogs:\n",
    "- Counts\n",
    "- Types\n",
    "- Frequent Items/Frequent Strings\n",
    "- Cardinality\n",
    "\n",
    "In this example, we'll see how you can track further metrics for string columns. We will do that by counting, for each string record, the number of characters that fall in a given unicode range, and then generating distribution metrics, such as `mean`, `stddev` and quantile values based on these counts. In addition to specific unicode ranges, we'll do the same approach, but for the overall string length.\n",
    "\n",
    "For more info on the unicode list of characters, check this [Wikipedia Article](https://en.wikipedia.org/wiki/List_of_Unicode_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.schema import ColumnSchema, DatasetSchema\n",
    "from whylogs.core.metrics.unicode_range import UnicodeRangeMetric\n",
    "from whylogs.core.resolvers import Resolver\n",
    "from whylogs.core.datatypes import DataType\n",
    "from typing import Dict\n",
    "from whylogs.core.metrics import Metric, MetricConfig\n",
    "\n",
    "class UnicodeResolver(Resolver):\n",
    "    def resolve(self, name: str, why_type: DataType, column_schema: ColumnSchema) -> Dict[str, Metric]:\n",
    "        return {UnicodeRangeMetric.get_namespace(): UnicodeRangeMetric.zero(column_schema.cfg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results = why.log(df_train.drop(\"embeddings\", axis=1), schema=DatasetSchema(resolvers=UnicodeResolver()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>column</th>\n",
       "      <th>labels</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/UNKNOWN:cardinality/est</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/UNKNOWN:cardinality/lower_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/UNKNOWN:cardinality/upper_1</th>\n",
       "      <td>1.00005</td>\n",
       "      <td>1.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/UNKNOWN:counts/inf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/string_length:types/boolean</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/string_length:types/fractional</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/string_length:types/integral</th>\n",
       "      <td>271</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/string_length:types/object</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unicode_range/string_length:types/string</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "column                                                    labels  \\\n",
       "type                                          SummaryType.COLUMN   \n",
       "unicode_range/UNKNOWN:cardinality/est                        1.0   \n",
       "unicode_range/UNKNOWN:cardinality/lower_1                    1.0   \n",
       "unicode_range/UNKNOWN:cardinality/upper_1                1.00005   \n",
       "unicode_range/UNKNOWN:counts/inf                               0   \n",
       "...                                                          ...   \n",
       "unicode_range/string_length:types/boolean                      0   \n",
       "unicode_range/string_length:types/fractional                   0   \n",
       "unicode_range/string_length:types/integral                   271   \n",
       "unicode_range/string_length:types/object                       0   \n",
       "unicode_range/string_length:types/string                       0   \n",
       "\n",
       "column                                                     words  \n",
       "type                                          SummaryType.COLUMN  \n",
       "unicode_range/UNKNOWN:cardinality/est                        1.0  \n",
       "unicode_range/UNKNOWN:cardinality/lower_1                    1.0  \n",
       "unicode_range/UNKNOWN:cardinality/upper_1                1.00005  \n",
       "unicode_range/UNKNOWN:counts/inf                               0  \n",
       "...                                                          ...  \n",
       "unicode_range/string_length:types/boolean                      0  \n",
       "unicode_range/string_length:types/fractional                   0  \n",
       "unicode_range/string_length:types/integral                   271  \n",
       "unicode_range/string_length:types/object                       0  \n",
       "unicode_range/string_length:types/string                       0  \n",
       "\n",
       "[253 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_results.profile().view().to_pandas().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example data is fairly clean, so there isn't as much interesting content, but the above would notify us of any emoticons, control characters, numerals, and extended Latin that unexpectedly shows up in production data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding references for embeddings\n",
    "\n",
    "We would like to compare incoming embeddings against up to 30 predefined references. These can chosen by the user either manually or algorithmically. Here, we use a supervised method for finding references, but some use cases dictate an unsupervised approach shown as well.\n",
    "\n",
    "For use with labeled training data (even if no labels at inference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.experimental.preprocess.embeddings.selectors import PCACentroidsSelector\n",
    "\n",
    "references, labels = PCACentroidsSelector(n_components=20).calculate_references(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use with unlabeled training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.experimental.preprocess.embeddings.selectors import PCAKMeansSelector\n",
    "\n",
    "unsup_references, _ = PCAKMeansSelector(n_clusters=8, n_components=20).calculate_references(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whylogs as why\n",
    "from whylogs.core.resolvers import MetricSpec, ResolverSpec\n",
    "from whylogs.core.schema import DeclarativeSchema\n",
    "from whylogs.experimental.core.metrics.embedding_metric import (\n",
    "    DistanceFunction,\n",
    "    EmbeddingConfig,\n",
    "    EmbeddingMetric,\n",
    ")\n",
    "\n",
    "config = EmbeddingConfig(\n",
    "    references=references,\n",
    "    labels=labels,\n",
    "    distance_fn=DistanceFunction.euclidean,\n",
    ")\n",
    "schema = DeclarativeSchema([ResolverSpec(column_name=\"embeddings\", metrics=[MetricSpec(EmbeddingMetric, config)])])\n",
    "\n",
    "train_profile = why.log(row={\"embeddings\": X_train}, schema=schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the contents of our profile measures the distribution of embeddings relative to the references we've provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>column</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>embedding/closest:counts/inf</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/closest:counts/n</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/closest:counts/nan</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/closest:counts/null</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/closest:frequent_items/frequent_strings</th>\n",
       "      <td>[FrequentItem(value='food', est=89, upper=89, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/sports_distance:types/fractional</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/sports_distance:types/integral</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/sports_distance:types/object</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding/sports_distance:types/string</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "column                                                                                    embeddings\n",
       "embedding/closest:counts/inf                                                                       0\n",
       "embedding/closest:counts/n                                                                       271\n",
       "embedding/closest:counts/nan                                                                       0\n",
       "embedding/closest:counts/null                                                                      0\n",
       "embedding/closest:frequent_items/frequent_strings  [FrequentItem(value='food', est=89, upper=89, ...\n",
       "...                                                                                              ...\n",
       "embedding/sports_distance:types/fractional                                                       271\n",
       "embedding/sports_distance:types/integral                                                           0\n",
       "embedding/sports_distance:types/object                                                             0\n",
       "embedding/sports_distance:types/string                                                             0\n",
       "type                                                                              SummaryType.COLUMN\n",
       "\n",
       "[126 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile.profile().view().to_pandas().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring embeddings drift in WhyLabs\n",
    "\n",
    "Both approaches can be really powerful for measuring drift across new batches of embeddings in a programmatic way using drift metrics as well as the WhyLabs Observability Platform.\n",
    "\n",
    "Consider changing our production data such that it represents drift. For example, perhaps the embeddings are purposely or accidentally rounded to the first decimal place. Perhaps we remove one of the categories, say sport, found in production. You can also add your own words to induce drift.\n",
    "\n",
    "Many similar issues get added to an ML pipeline and will have a detrimental impact on our incoming data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "### Upload profiles to WhyLabs for more drift calculations and monitoring\n",
    "\n",
    "See [example notebook](https://whylogs.readthedocs.io/en/stable/examples/integrations/writers/Writing_to_WhyLabs.html) for monitoring your profiles continuously with the WhyLabs Observability Platform.\n",
    "\n",
    "### Exploring other sources of drift\n",
    "\n",
    "Consider comparing this profile to different transformations and subsets of our MNIST dataset: randomly selected subsets of the data, normalized values, missing one or more labels, sorted values, and more.\n",
    "\n",
    "### More example notebooks and documentation\n",
    "\n",
    "Go to the [examples page](https://whylogs.readthedocs.io/en/stable/examples.html) for the complete list of examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wl-public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb094116d37501200f308f033158433bea171caabc5df0ae059b169e583875a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
