{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
    ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Segments)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Segments) to leverage the power of whylogs and WhyLabs together!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columnar Segmented Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/advanced/Segments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing whylogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have it installed already, install whylogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'whylogs[whylabs]' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data & Defining the Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the data we'll be working with.\n",
    "\n",
    "This dataset contains transaction information for an online grocery store, such as:\n",
    "\n",
    "- product description\n",
    "- category\n",
    "- user rating\n",
    "- market price\n",
    "- number of items sold last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "DEMO_LENGTH_IN_DAYS = 8\n",
    "ANOMALY_RATIO_RANDOM = 0.15\n",
    "SKIP_PROFILING_SEGMENT_INPUTS = False\n",
    "USE_CLASSIFICATION = True\n",
    "\n",
    "url = \"https://whylabs-public.s3.us-west-2.amazonaws.com/whylogs_examples/Ecommerce/baseline_dataset_base.csv\"\n",
    "df = pd.read_csv(url)[[\"date\",\"product\",\"category\", \"rating\", \"market_price\",\"sales_last_week\"]]\n",
    "df['rating'] = df['rating'].astype(int)\n",
    "\n",
    "# Here we simulate some kind of prediction, this is a crude prediction that is usually correct but\n",
    "# guesses when actual rating is 5 (which are a relatively small portion of all product rating values)\n",
    "def predict_rating(actual_rating):\n",
    "    prediction = actual_rating\n",
    "    if actual_rating < 4:\n",
    "        if random.randint(1,100) > 98:\n",
    "            prediction = actual_rating + 1\n",
    "    else:\n",
    "        prediction = random.randint(1,5)\n",
    "    return prediction\n",
    "\n",
    "df['predicted_rating'] = df['rating'].apply(predict_rating)\n",
    "# This simulates an anomaly (change in model performance) that starts after 'error_starts_days_ago'\n",
    "# back in time (e.g. with a value of 3 here, the most recent 2 days have degraded model performance)\n",
    "error_starts_days_ago = 3\n",
    "\n",
    "\n",
    "# now let's split the data into n batches randomly to mimic different days of data\n",
    "n=DEMO_LENGTH_IN_DAYS\n",
    "batches = np.array_split(df.sample(frac=1), n)\n",
    "\n",
    "# here is the additional error we are injecting to create an anomaly: randomize the prediced\n",
    "# rating 15% of the time if ANOMALY_RATIO_RANDOM = 0.15\n",
    "def inject_more_error(rating):\n",
    "    if random.random() > (1.0-ANOMALY_RATIO_RANDOM):\n",
    "        return random.randint(1,5)\n",
    "    return rating\n",
    "\n",
    "days_ago_count = 0\n",
    "for df in batches:\n",
    "    days_ago_count = days_ago_count + 1\n",
    "    if days_ago_count < error_starts_days_ago:\n",
    "        df['predicted_rating'] = df['predicted_rating'].apply(inject_more_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"single\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the API and env vars for upload to WhyLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's upload the unsegmented data profile to WhyLabs, set the env variables\n",
    "import getpass\n",
    "import os\n",
    "import whylogs as why\n",
    "\n",
    "\n",
    "# set your org-id here - should be something like \"org-xxxx\"\n",
    "print(\"Enter your WhyLabs Org ID\") \n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input()\n",
    "print(\"Enter the model-id\")\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] =  input()\n",
    "print(\"Enter your WhyLabs API Key\")\n",
    "os.environ[\"WHYLABS_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting on a Single Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say you have a few columns that you want to segment on, but you don't want to see the cartesian product of those columns. In this example we will choose the column `category` and then the column `rating` as interesting features to segment on and calculate performance metrics on these segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import List\n",
    "import whylogs as why\n",
    "from whylogs.core.schema import DatasetSchema\n",
    "from whylogs.core.segmentation_partition import segment_on_column\n",
    "from whylogs.core.dataset_profile import DatasetProfile\n",
    "from whylogs.core.model_performance_metrics.model_performance_metrics import ModelPerformanceMetrics\n",
    "\n",
    "# helper to support going from string segment key to typed group values in pandas\n",
    "def lookup_typed_key(grouped_pdf, key: str):\n",
    "    typed_keys = set(grouped_pdf.indices.keys())\n",
    "    for typed_key in typed_keys:\n",
    "        if key == str(typed_key):\n",
    "            return typed_key\n",
    "    return key\n",
    "\n",
    "#create a helper method to add performance metrics to segmented pandas dataframes\n",
    "def add_performance_to_segments(grouped_pdf, prediction_column, target_column, results):\n",
    "    partition = results.partitions[0]\n",
    "    segments = results.segments_in_partition(partition)\n",
    "    for segment in segments:\n",
    "        # A segment's key is a tuple of the column values, since we segmented on a single column\n",
    "        # the first value in the tuple will be the same as a groupby key in pandas ('Baby care',),\n",
    "        # so get that the pandas group using this key. e.g. \"Baby care\" etc\n",
    "        typed_key = lookup_typed_key(grouped_pdf, segment.key[0])\n",
    "\n",
    "        segmented_pdf = grouped_pdf.get_group(typed_key)\n",
    "        perf = ModelPerformanceMetrics()\n",
    "        if USE_CLASSIFICATION:\n",
    "            perf.compute_confusion_matrix(\n",
    "                predictions=segmented_pdf[prediction_column].to_list(),\n",
    "                targets=segmented_pdf[target_column].to_list(),\n",
    "            )\n",
    "        else:\n",
    "            perf.compute_regression_metrics(\n",
    "                predictions=segmented_pdf[prediction_column].to_list(),\n",
    "                targets=segmented_pdf[target_column].to_list(),\n",
    "            )\n",
    "\n",
    "        if SKIP_PROFILING_SEGMENT_INPUTS:\n",
    "            #create a blank profile for the segment so that we only have the model performance.\n",
    "            segments[segment] = DatasetProfile()\n",
    "\n",
    "        segmented_profile = results.profile(segment)\n",
    "        segmented_profile.add_model_performance_metrics(perf)\n",
    "\n",
    "# helper method to profile and upload segmented perf metrics for each column\n",
    "# (non-cartesian product) for the last n days\n",
    "def upload_columnar_segmented_performance_for_past_n_days(\n",
    "    bathes,\n",
    "    segment_columns: List[str],\n",
    "    prediction_column: str,\n",
    "    target_column: str,\n",
    "    n: int = 7):\n",
    "    for column_name in segment_columns:\n",
    "        print(f\"** Profiling segmented performance metrics for column({column_name}) \"\n",
    "              f\"using targets ({target_column}) and predictions ({prediction_column})\")\n",
    "        # split the pandas data frame on the same column that segmented profiles used\n",
    "\n",
    "        for i in range(n):\n",
    "            df = batches[i]\n",
    "            grouped_pdf = df.groupby(column_name)\n",
    "            dt = datetime.datetime.now(tz=datetime.timezone.utc) - datetime.timedelta(days=i)\n",
    "            print(f\"    About to log data for {i} days ago-> {dt}\")\n",
    "            results = why.log(df, schema=DatasetSchema(segments=segment_on_column(column_name)))\n",
    "            print(f\"    Segmented profile for {column_name} on {dt} has {results.count} segments\")\n",
    "            add_performance_to_segments(grouped_pdf, prediction_column, target_column, results)\n",
    "            results.set_dataset_timestamp(dt)\n",
    "            print(f\"    Uploading profiles for {column_name} on {dt}, this may take a while...\")\n",
    "            results.writer(\"whylabs\").write()\n",
    "            print(f\"    Done uploading profiles for {column_name} on {dt}\")\n",
    "    print(f\"** Done uploading profiles!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the column(s) you want to segment on here\n",
    "# as well as the target and prediction columns\n",
    "columns = [\"category\", \"rating\"]\n",
    "prediction_column = 'predicted_rating'\n",
    "target_column = 'rating'\n",
    "\n",
    "upload_columnar_segmented_performance_for_past_n_days(\n",
    "    batches, columns, prediction_column, target_column, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d39f874c9b8a97550ecbd783714b95e79c9b905449b34f44c40e3bf053b54b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
