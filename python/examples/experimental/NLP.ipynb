{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUsdUYUbNrpl"
      },
      "source": [
        ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
        ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=String_Tracking)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=String_Tracking) to leverage the power of whylogs and WhyLabs together!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1-M8tfxNrpn"
      },
      "source": [
        "# Natural Language Processing Logging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6cXXyVONrpo"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/advanced/String_Tracking.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlasZRJiNrpo"
      },
      "source": [
        "Blah blah blah NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRGvUCCUNrpq"
      },
      "source": [
        "## Installing whylogs\n",
        "\n",
        "If you haven't already, install whylogs: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install whylogs-1.1.7-py3-none-any.whl"
      ],
      "metadata": {
        "id": "z3PdcQj4yUq2",
        "outputId": "ac289aa8-0a54-4b4e-ec02-25c1f5793cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./whylogs-1.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata<4.3 in /usr/local/lib/python3.7/dist-packages (from whylogs==1.1.7) (4.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10 in /usr/local/lib/python3.7/dist-packages (from whylogs==1.1.7) (4.1.1)\n",
            "Requirement already satisfied: whylogs-sketching>=3.4.1.dev3 in /usr/local/lib/python3.7/dist-packages (from whylogs==1.1.7) (3.4.1.dev3)\n",
            "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.7/dist-packages (from whylogs==1.1.7) (3.19.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->whylogs==1.1.7) (3.10.0)\n",
            "whylogs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG-k1QJfNrpr"
      },
      "outputs": [],
      "source": [
        "%pip install whylogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdfAioiRNrps"
      },
      "source": [
        "## Creating the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7VC5LX8Nrpt"
      },
      "source": [
        "We'll install NLTK to get access to its corpora and basic NLP functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "id": "E4LOWIVnautP",
        "outputId": "b9155fd4-98cf-488b-e8f3-38df7cbaf0d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by building an inverted index of the NLTK inaugural corpus. We'll use NLTK's facilities for tokenization, stemming, and stopping. We'll use log-entropy weighting."
      ],
      "metadata": {
        "id": "7xggMfh1dTrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import inaugural, stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from whylogs.core.configs import SummaryConfig\n",
        "from whylogs.experimental.core.metrics.nlp_metric import (\n",
        "    NlpLogger,\n",
        "    SvdMetric,\n",
        "    SvdMetricConfig,\n",
        "    UpdatableSvdMetric,\n",
        ")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('inaugural')\n",
        "\n",
        "# inverted index weighting utility functions\n",
        "\n",
        "\n",
        "def global_freq(A: np.ndarray) -> np.ndarray:\n",
        "    gf = np.zeros(A.shape[0])\n",
        "    for i in range(A.shape[0]):\n",
        "        for j in range(A.shape[1]):\n",
        "            gf[i] += A[i, j]\n",
        "    return gf\n",
        "\n",
        "\n",
        "def entropy(A: np.ndarray) -> np.ndarray:\n",
        "    gf = global_freq(A)\n",
        "    g = np.ones(A.shape[0])\n",
        "    logN = np.log(A.shape[1])\n",
        "    assert logN > 0.0\n",
        "    for i in range(A.shape[0]):\n",
        "        assert gf[i] > 0.0\n",
        "        for j in range(A.shape[1]):\n",
        "            p_ij = A[i, j] / gf[i]\n",
        "            g[i] += p_ij * np.log(p_ij) / logN if p_ij > 0.0 else 0.0\n",
        "    return g\n",
        "\n",
        "\n",
        "def log_entropy(A: np.ndarray) -> None:\n",
        "    g = entropy(A)\n",
        "    for i in range(A.shape[0]):\n",
        "        for j in range(A.shape[1]):\n",
        "            A[i, j] = g[i] * np.log(A[i, j] + 1.0)\n",
        "\n",
        "\n",
        "# the NLTK tokenizer produces punctuation as terms, so stop them\n",
        "stop_words = set(\n",
        "    stopwords.words(\"english\")\n",
        "    + [\n",
        "        \".\",\n",
        "        \",\",\n",
        "        \":\",\n",
        "        \";\",\n",
        "        '.\"',\n",
        "        ',\"',\n",
        "        '\"',\n",
        "        \"'\",\n",
        "        \" \",\n",
        "        \"?\",\n",
        "        \"[\",\n",
        "        \"]\",\n",
        "        \".]\",\n",
        "        \"' \",\n",
        "        '\" ',\n",
        "        \"? \",\n",
        "        \"-\",\n",
        "        \"- \",\n",
        "        \"/\",\n",
        "        '?\"',\n",
        "        \"...\",\n",
        "        \"\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# build weighted inverted index of inaugural speeches\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "vstopped = {w for w in inaugural.words() if w.casefold() not in stop_words}\n",
        "vocab = {stemmer.stem(w.casefold()) for w in vstopped}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "vocab_map = {}\n",
        "rev_map = [\"\"] * vocab_size\n",
        "dim = 0\n",
        "for w in vocab:\n",
        "    if w not in vocab_map:\n",
        "        vocab_map[w] = dim\n",
        "        rev_map[dim] = w\n",
        "        dim += 1\n",
        "\n",
        "doc_lengths = []\n",
        "ndocs = len(inaugural.fileids())\n",
        "doc = 0\n",
        "index = np.zeros((vocab_size, ndocs))\n",
        "for fid in inaugural.fileids():\n",
        "    stopped = [t.casefold() for t in inaugural.words(fid) if t.casefold() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in stopped]\n",
        "    doc_lengths.append(len(stemmed))\n",
        "    for w in stemmed:\n",
        "        index[vocab_map[w], doc] += 1\n",
        "    doc += 1\n",
        "\n",
        "# A is our weighted inverted index\n",
        "A = index.copy()\n",
        "log_entropy(A)\n",
        "\n",
        "# We'll need the global frequencies and entropies for weighting new document vectors\n",
        "gf = global_freq(index)\n",
        "g = entropy(index)"
      ],
      "metadata": {
        "id": "kmdFZkcyblgp",
        "outputId": "493c41bc-4643-49cb-f2a7-5cc54927face",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log the reference profile\n",
        "\n",
        "Let's instantiate an `NlpLogger` to create the reference profile for the corpus. We need to pick the number of eigenvalues to keep in the SVD approximation of the inverted index. Since the inaugural corpus is very small, we'll set it to 10. We also need to specify the `UpdatableSvdMetric` since we want to update the SVD approximation as we process the documents.\n",
        "\n",
        "Once we've logged the documents, we can send the profile to whylabs and save the SVD locally."
      ],
      "metadata": {
        "id": "FzwQ4po6ePHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_concepts = 10\n",
        "old_doc_decay_rate = 1.0\n",
        "svd_config = SvdMetricConfig(k=num_concepts, decay=old_doc_decay_rate)\n",
        "nlp_logger = NlpLogger(svd_class=UpdatableSvdMetric, svd_config=svd_config)\n",
        "\n",
        "for fid in inaugural.fileids():\n",
        "    stopped = [t.casefold() for t in inaugural.words(fid) if t.casefold() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in stopped]\n",
        "\n",
        "    doc_vec = np.zeros(vocab_size)\n",
        "    for w in stemmed:\n",
        "        doc_vec[vocab_map[w]] += 1\n",
        "    for i in range(vocab_size):\n",
        "        doc_vec[i] = g[i] * np.log(doc_vec[i] + 1.0)\n",
        "\n",
        "    nlp_logger.log(stemmed, doc_vec)\n",
        "\n",
        "\n",
        "# save reference profile locally\n",
        "send_me_to_whylabs = nlp_logger.get_profile()  # small--only has a few standard metrics (no SVD)\n",
        "nlp_logger._profile.flush()\n",
        "svd_write_me = nlp_logger.get_svd_state()  # big--contains the SVD approximation & parameters\n"
      ],
      "metadata": {
        "id": "SSXRdt62ePqe",
        "outputId": "3a305155-887e-4660-8e96-edb3278479a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c3aede1c1917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# save reference profile locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msend_me_to_whylabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# small--only has a few standard metrics (no SVD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mnlp_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0msvd_write_me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_svd_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# big--contains the SVD approximation & parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whylogs/core/dataset_profile.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whylogs/core/column_profile.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mold_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrack_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whylogs/core/column_profile.py\u001b[0m in \u001b[0;36mtrack_column\u001b[0;34m(self, series)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessedColumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumnar_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_failure_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_success_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whylogs/experimental/core/metrics/nlp_metric.py\u001b[0m in \u001b[0;36mcolumnar_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# data.list.objs is as list of np.ndarray. Each ndarray represents one document's term vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolumnar_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPreprocessedColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOperationResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumnar_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# no-op if SVD is not updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0mresiduals\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: batch these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whylogs/experimental/core/metrics/nlp_metric.py\u001b[0m in \u001b[0;36mcolumnar_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mvectors_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at the resulting profile"
      ],
      "metadata": {
        "id": "b7M_6BMZfC2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp_logger.get_profile().profile()._columns.keys())\n",
        "\n",
        "svd = nlp_logger._svd_metric\n",
        "\n",
        "concepts = svd.U.value.transpose()\n",
        "for i in range(concepts.shape[0]):\n",
        "    pos_idx = sorted(range(len(concepts[i])), key=lambda x: concepts[i][x])[-10:]\n",
        "    neg_idx = sorted(range(len(concepts[i])), key=lambda x: -1 * concepts[i][x])[-5:]\n",
        "    print(\", \".join([rev_map[j] for j in pos_idx]))  # + [rev_map[j] for j in neg_idx]))\n",
        "print()"
      ],
      "metadata": {
        "id": "k_acBpdEfDu0",
        "outputId": "e0b625b5-7076-432f-860a-66cd379a6f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['nlp_bag_of_words', 'nlp_lsi'])\n",
            "reunion, annex, bill, occasion, compromis, array, discrimin, european, texa, tax\n",
            "hate, exig, mischief, proposit, speedili, exercis, discrimin, minor, texa, union\n",
            "!, outset, actual, unequ, amid, fortitud, overlook, outrun, weigh, forebod\n",
            "ballot, unless, journey, job, seced, case, stori, slaveri, fugit, Â€Â”\n",
            "deleg, occasion, incident, discrimin, feder, reunion, annex, compromis, levi, texa\n",
            "preced, invas, railroad, predecessor, board, type, negro, fortif, coast, interst\n",
            "tribe, florida, contest, occurr, neutral, spain, union, naval, territori, augment\n",
            "philippin, suitabl, elector, report, employe, tariff, south, type, negro, interst\n",
            "era, journey, world, econom, Â€Â”, challeng, help, ideal, today, america\n",
            "legisl, power, execut, revenu, congress, upon, law, union, state, constitut\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For production logging, we can choose whether or not to continue updating the SVD approximation. In this case, we'll use `SvdMetric` so that the reference SVD won't be updated. We'll load the reference SVD that we saved locally."
      ],
      "metadata": {
        "id": "mya8scqGfUWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# production tracking, no reference update\n",
        "\n",
        "prod_logger = NlpLogger(svd_class=SvdMetric, svd_state=svd_write_me)  # use UpdatableSvdMetric to train in production\n",
        "\n",
        "prod_svd = prod_logger._svd_metric\n",
        "\n",
        "residuals = []\n",
        "for fid in inaugural.fileids():\n",
        "    stopped = [t.casefold() for t in inaugural.words(fid) if t.casefold() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in stopped]\n",
        "\n",
        "    doc_vec = np.zeros(vocab_size)\n",
        "    for w in stemmed:\n",
        "        doc_vec[vocab_map[w]] += 1\n",
        "    for i in range(vocab_size):\n",
        "        doc_vec[i] = g[i] * np.log(doc_vec[i] + 1.0)\n",
        "\n",
        "    residuals.append(prod_svd.residual(doc_vec))\n",
        "    prod_logger.log(stemmed, doc_vec)  # update residual only, not SVD\n",
        "\n",
        "print(f\"\\nresiduals: {residuals}\\n\")\n",
        "\n",
        "# if we trained with production data\n",
        "# svd_write_me = prod_logger.get_svd_state()\n",
        "\n",
        "# send to whylabs, no SVD state\n",
        "send_me = prod_logger.get_profile()\n",
        "\n",
        "# get stats on doc length, term length, SVD \"fit\"\n",
        "view_me = prod_svd.to_summary_dict(SummaryConfig())\n",
        "print(view_me)\n"
      ],
      "metadata": {
        "id": "DrwUYzRufU_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('v1.x')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f76ec28949fecf16b926a3fc5a03c1aa6468ee82fa5da4ce6fd607df021af5b5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}