{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUsdUYUbNrpl"
      },
      "source": [
        ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
        ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=String_Tracking)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=String_Tracking) to leverage the power of whylogs and WhyLabs together!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1-M8tfxNrpn"
      },
      "source": [
        "# Natural Language Processing Logging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6cXXyVONrpo"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/advanced/String_Tracking.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlasZRJiNrpo"
      },
      "source": [
        "Blah blah blah NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRGvUCCUNrpq"
      },
      "source": [
        "## Installing whylogs\n",
        "\n",
        "If you haven't already, install whylogs: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install whylogs-1.1.7-py3-none-any.whl"
      ],
      "metadata": {
        "id": "z3PdcQj4yUq2",
        "outputId": "1e35dea7-09c9-407a-c84f-d5ac612f4c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./whylogs-1.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.10 in /usr/local/lib/python3.7/dist-packages (from whylogs==1.1.7) (4.1.1)\n",
            "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.7/dist-packages (from whylogs==1.1.7) (3.19.6)\n",
            "Collecting whylogs-sketching>=3.4.1.dev3\n",
            "  Downloading whylogs_sketching-3.4.1.dev3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (559 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 559 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.3\n",
            "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->whylogs==1.1.7) (3.10.0)\n",
            "Installing collected packages: whylogs-sketching, importlib-metadata, whylogs\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.13.0\n",
            "    Uninstalling importlib-metadata-4.13.0:\n",
            "      Successfully uninstalled importlib-metadata-4.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\u001b[0m\n",
            "Successfully installed importlib-metadata-4.2.0 whylogs-1.1.7 whylogs-sketching-3.4.1.dev3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG-k1QJfNrpr"
      },
      "outputs": [],
      "source": [
        "%pip install whylogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdfAioiRNrps"
      },
      "source": [
        "## Creating the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7VC5LX8Nrpt"
      },
      "source": [
        "We'll install NLTK to get access to its corpora and basic NLP functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "id": "E4LOWIVnautP",
        "outputId": "66d63b33-7238-44db-e431-db263297749c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by building an inverted index of the NLTK inaugural corpus. We'll use NLTK's facilities for tokenization, stemming, and stopping. We'll use log-entropy weighting."
      ],
      "metadata": {
        "id": "7xggMfh1dTrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import inaugural, stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from whylogs.core.configs import SummaryConfig\n",
        "from whylogs.experimental.core.metrics.nlp_metric import (\n",
        "    NlpLogger,\n",
        "    SvdMetric,\n",
        "    SvdMetricConfig,\n",
        "    UpdatableSvdMetric,\n",
        ")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('inaugural')\n",
        "\n",
        "# inverted index weighting utility functions\n",
        "\n",
        "\n",
        "def global_freq(A: np.ndarray) -> np.ndarray:\n",
        "    gf = np.zeros(A.shape[0])\n",
        "    for i in range(A.shape[0]):\n",
        "        for j in range(A.shape[1]):\n",
        "            gf[i] += A[i, j]\n",
        "    return gf\n",
        "\n",
        "\n",
        "def entropy(A: np.ndarray) -> np.ndarray:\n",
        "    gf = global_freq(A)\n",
        "    g = np.ones(A.shape[0])\n",
        "    logN = np.log(A.shape[1])\n",
        "    assert logN > 0.0\n",
        "    for i in range(A.shape[0]):\n",
        "        assert gf[i] > 0.0\n",
        "        for j in range(A.shape[1]):\n",
        "            p_ij = A[i, j] / gf[i]\n",
        "            g[i] += p_ij * np.log(p_ij) / logN if p_ij > 0.0 else 0.0\n",
        "    return g\n",
        "\n",
        "\n",
        "def log_entropy(A: np.ndarray) -> None:\n",
        "    g = entropy(A)\n",
        "    for i in range(A.shape[0]):\n",
        "        for j in range(A.shape[1]):\n",
        "            A[i, j] = g[i] * np.log(A[i, j] + 1.0)\n",
        "\n",
        "\n",
        "# the NLTK tokenizer produces punctuation as terms, so stop them\n",
        "stop_words = set(\n",
        "    stopwords.words(\"english\")\n",
        "    + [\n",
        "        \".\",\n",
        "        \",\",\n",
        "        \":\",\n",
        "        \";\",\n",
        "        '.\"',\n",
        "        ',\"',\n",
        "        '\"',\n",
        "        \"'\",\n",
        "        \" \",\n",
        "        \"?\",\n",
        "        \"[\",\n",
        "        \"]\",\n",
        "        \".]\",\n",
        "        \"' \",\n",
        "        '\" ',\n",
        "        \"? \",\n",
        "        \"-\",\n",
        "        \"- \",\n",
        "        \"/\",\n",
        "        '?\"',\n",
        "        \"...\",\n",
        "        \"\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# build weighted inverted index of inaugural speeches\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "vstopped = {w for w in inaugural.words() if w.casefold() not in stop_words}\n",
        "vocab = {stemmer.stem(w.casefold()) for w in vstopped}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "vocab_map = {}\n",
        "rev_map = [\"\"] * vocab_size\n",
        "dim = 0\n",
        "for w in vocab:\n",
        "    if w not in vocab_map:\n",
        "        vocab_map[w] = dim\n",
        "        rev_map[dim] = w\n",
        "        dim += 1\n",
        "\n",
        "doc_lengths = []\n",
        "ndocs = len(inaugural.fileids())\n",
        "doc = 0\n",
        "index = np.zeros((vocab_size, ndocs))\n",
        "for fid in inaugural.fileids():\n",
        "    stopped = [t.casefold() for t in inaugural.words(fid) if t.casefold() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in stopped]\n",
        "    doc_lengths.append(len(stemmed))\n",
        "    for w in stemmed:\n",
        "        index[vocab_map[w], doc] += 1\n",
        "    doc += 1\n",
        "\n",
        "# A is our weighted inverted index\n",
        "A = index.copy()\n",
        "log_entropy(A)\n",
        "\n",
        "# We'll need the global frequencies and entropies for weighting new document vectors\n",
        "gf = global_freq(index)\n",
        "g = entropy(index)"
      ],
      "metadata": {
        "id": "kmdFZkcyblgp",
        "outputId": "d8d3974b-65c8-4207-db45-e8916c53cdae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log the reference profile\n",
        "\n",
        "Let's instantiate an `NlpLogger` to create the reference profile for the corpus. We need to pick the number of eigenvalues to keep in the SVD approximation of the inverted index. Since the inaugural corpus is very small, we'll set it to 10. We also need to specify the `UpdatableSvdMetric` since we want to update the SVD approximation as we process the documents.\n",
        "\n",
        "Once we've logged the documents, we can send the profile to whylabs and save the SVD locally."
      ],
      "metadata": {
        "id": "FzwQ4po6ePHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_concepts = 10\n",
        "old_doc_decay_rate = 1.0\n",
        "svd_config = SvdMetricConfig(k=num_concepts, decay=old_doc_decay_rate)\n",
        "nlp_logger = NlpLogger(svd_class=UpdatableSvdMetric, svd_config=svd_config)\n",
        "\n",
        "for fid in inaugural.fileids():\n",
        "    stopped = [t.casefold() for t in inaugural.words(fid) if t.casefold() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in stopped]\n",
        "\n",
        "    doc_vec = np.zeros(vocab_size)\n",
        "    for w in stemmed:\n",
        "        doc_vec[vocab_map[w]] += 1\n",
        "    for i in range(vocab_size):\n",
        "        doc_vec[i] = g[i] * np.log(doc_vec[i] + 1.0)\n",
        "\n",
        "    nlp_logger.log(stemmed, doc_vec)\n",
        "\n",
        "\n",
        "# save reference profile locally\n",
        "send_me_to_whylabs = nlp_logger.get_profile()  # small--only has a few standard metrics (no SVD)\n",
        "nlp_logger._profile.flush()\n",
        "svd_write_me = nlp_logger.get_svd_state()  # big--contains the SVD approximation & parameters\n"
      ],
      "metadata": {
        "id": "SSXRdt62ePqe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at the resulting profile"
      ],
      "metadata": {
        "id": "b7M_6BMZfC2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_summary = nlp_logger.get_profile().view().get_column(\"nlp_bag_of_words\").get_metric(\"nlp_bow\").to_summary_dict()\n",
        "for key, value in bow_summary.items():\n",
        "  print(f\"  {key}: {value}\")\n",
        "print()\n",
        "\n",
        "svd = nlp_logger._svd_metric\n",
        "concepts = svd.U.value.transpose()\n",
        "for i in range(concepts.shape[0]):\n",
        "    pos_idx = sorted(range(len(concepts[i])), key=lambda x: concepts[i][x])[-10:]\n",
        "    neg_idx = sorted(range(len(concepts[i])), key=lambda x: -1 * concepts[i][x])[-5:]\n",
        "    print(\", \".join([rev_map[j] for j in pos_idx]))  # + [rev_map[j] for j in neg_idx]))\n",
        "print()"
      ],
      "metadata": {
        "id": "k_acBpdEfDu0",
        "outputId": "dc2c46f9-d703-4d48-e65b-2638f84272e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  doc_length:distribuion/mean: 1121.0847457627117\n",
            "  doc_length:distribuion/stddev: 638.2022781742044\n",
            "  doc_length:distribuion/n: 59\n",
            "  doc_length:distribuion/max: 3833.0\n",
            "  doc_length:distribuion/min: 62.0\n",
            "  doc_length:distribuion/q_01: 62.0\n",
            "  doc_length:distribuion/q_05: 343.0\n",
            "  doc_length:distribuion/q_10: 524.0\n",
            "  doc_length:distribuion/q_25: 669.0\n",
            "  doc_length:distribuion/median: 1030.0\n",
            "  doc_length:distribuion/q_75: 1370.0\n",
            "  doc_length:distribuion/q_90: 1940.0\n",
            "  doc_length:distribuion/q_95: 2280.0\n",
            "  doc_length:distribuion/q_99: 3833.0\n",
            "  doc_length:counts/n: 59\n",
            "  doc_length:counts/null: 0\n",
            "  doc_length:types/integral: 59\n",
            "  doc_length:types/fractional: 0\n",
            "  doc_length:types/boolean: 0\n",
            "  doc_length:types/string: 0\n",
            "  doc_length:types/object: 0\n",
            "  doc_length:cardinality/est: 57.00000792741905\n",
            "  doc_length:cardinality/upper_1: 57.00285389510009\n",
            "  doc_length:cardinality/lower_1: 57.0\n",
            "  doc_length:ints/max: 3833\n",
            "  doc_length:ints/min: 62\n",
            "  term_length:distribution/mean: 5.591935776487663\n",
            "  term_length:distribution/stddev: 1.6809668513701828\n",
            "  term_length:distribution/n: 66144\n",
            "  term_length:distribution/max: 16.0\n",
            "  term_length:distribution/min: 1.0\n",
            "  term_length:distribution/q_01: 2.0\n",
            "  term_length:distribution/q_05: 3.0\n",
            "  term_length:distribution/q_10: 4.0\n",
            "  term_length:distribution/q_25: 4.0\n",
            "  term_length:distribution/median: 5.0\n",
            "  term_length:distribution/q_75: 7.0\n",
            "  term_length:distribution/q_90: 8.0\n",
            "  term_length:distribution/q_95: 9.0\n",
            "  term_length:distribution/q_99: 10.0\n",
            "  term_length:counts/n: 66144\n",
            "  term_length:counts/null: 0\n",
            "  term_length:types/integral: 66144\n",
            "  term_length:types/fractional: 0\n",
            "  term_length:types/boolean: 0\n",
            "  term_length:types/string: 0\n",
            "  term_length:types/object: 0\n",
            "  term_length:cardinality/est: 16.000000596046476\n",
            "  term_length:cardinality/upper_1: 16.000799464086125\n",
            "  term_length:cardinality/lower_1: 16.0\n",
            "  term_length:ints/max: 16\n",
            "  term_length:ints/min: 1\n",
            "  frequent_terms:frequent_items/frequent_strings: [FrequentItem(value='nation', est=692, upper=613, lower=692), FrequentItem(value='govern', est=687, upper=608, lower=687), FrequentItem(value='peopl', est=633, upper=554, lower=633), FrequentItem(value='us', est=502, upper=423, lower=502), FrequentItem(value='state', est=453, upper=374, lower=453), FrequentItem(value='power', est=375, upper=296, lower=375), FrequentItem(value='must', est=374, upper=295, lower=374), FrequentItem(value='upon', est=371, upper=292, lower=371), FrequentItem(value='--', est=363, upper=284, lower=363), FrequentItem(value='great', est=361, upper=282, lower=361), FrequentItem(value='countri', est=359, upper=280, lower=359), FrequentItem(value='world', est=348, upper=269, lower=348), FrequentItem(value='may', est=343, upper=264, lower=343), FrequentItem(value='shall', est=316, upper=237, lower=316), FrequentItem(value='citizen', est=304, upper=225, lower=304), FrequentItem(value='everi', est=301, upper=222, lower=301), FrequentItem(value='constitut', est=289, upper=210, lower=289), FrequentItem(value='peac', est=288, upper=209, lower=288), FrequentItem(value='one', est=282, upper=203, lower=282), FrequentItem(value='law', est=278, upper=199, lower=278), FrequentItem(value='america', est=274, upper=195, lower=274), FrequentItem(value='time', est=274, upper=195, lower=274), FrequentItem(value='right', est=274, upper=195, lower=274), FrequentItem(value='new', est=260, upper=181, lower=260), FrequentItem(value='american', est=259, upper=180, lower=259), FrequentItem(value='public', est=228, upper=149, lower=228), FrequentItem(value='unit', est=225, upper=146, lower=225), FrequentItem(value='would', est=213, upper=134, lower=213), FrequentItem(value='duti', est=212, upper=133, lower=212), FrequentItem(value='war', est=207, upper=128, lower=207), FrequentItem(value='make', est=205, upper=126, lower=205), FrequentItem(value='interest', est=199, upper=120, lower=199), FrequentItem(value='freedom', est=196, upper=117, lower=196), FrequentItem(value='union', est=191, upper=112, lower=191), FrequentItem(value='free', est=185, upper=106, lower=185), FrequentItem(value='gener', est=184, upper=105, lower=184), FrequentItem(value='secur', est=178, upper=99, lower=178), FrequentItem(value='year', est=177, upper=98, lower=177), FrequentItem(value='hope', est=177, upper=98, lower=177), FrequentItem(value='let', est=166, upper=87, lower=166), FrequentItem(value='good', est=164, upper=85, lower=164), FrequentItem(value='work', est=162, upper=83, lower=162)]\n",
            "  frequent_terms:counts/n: 0\n",
            "  frequent_terms:counts/null: 0\n",
            "  frequent_terms:types/integral: 0\n",
            "  frequent_terms:types/fractional: 0\n",
            "  frequent_terms:types/boolean: 0\n",
            "  frequent_terms:types/string: 0\n",
            "  frequent_terms:types/object: 0\n",
            "  frequent_terms:cardinality/est: 5461.112420374854\n",
            "  frequent_terms:cardinality/upper_1: 5532.59538963055\n",
            "  frequent_terms:cardinality/lower_1: 5391.368315825156\n",
            "\n",
            "secess, imperfectli, case, fli, slave, claus, dissatisfi, unanim, seced, fugit\n",
            "silver, distract, foremost, 1890, trial, postpon, session, benefici, loan, conven\n",
            "settlement, reserv, indebt, loan, annex, reunion, gloriou, revenu, levi, texa\n",
            "stagger, reassur, acclaim, comiti, rivet, proven, hate, unshaken, normal, relationship\n",
            "loan, florida, occurr, ship, recollect, naval, fortif, conclud, coloni, spain\n",
            "preced, invas, railroad, predecessor, board, type, negro, fortif, coast, interst\n",
            "railroad, employe, roman, summari, busi, amend, type, bill, negro, interst\n",
            "philippin, suitabl, elector, report, employe, tariff, south, type, negro, interst\n",
            "era, journey, world, econom, Â€Â”, challeng, help, ideal, today, america\n",
            "legisl, power, execut, revenu, congress, upon, law, union, state, constitut\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For production logging, we can choose whether or not to continue updating the SVD approximation. In this case, we'll use `SvdMetric` so that the reference SVD won't be updated. We'll load the reference SVD that we saved locally."
      ],
      "metadata": {
        "id": "mya8scqGfUWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# production tracking, no reference update\n",
        "\n",
        "prod_logger = NlpLogger(svd_class=SvdMetric, svd_state=svd_write_me)  # use UpdatableSvdMetric to train in production\n",
        "\n",
        "prod_svd = prod_logger._svd_metric\n",
        "\n",
        "residuals = []\n",
        "for fid in inaugural.fileids():\n",
        "    stopped = [t.casefold() for t in inaugural.words(fid) if t.casefold() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in stopped]\n",
        "\n",
        "    doc_vec = np.zeros(vocab_size)\n",
        "    for w in stemmed:\n",
        "        doc_vec[vocab_map[w]] += 1\n",
        "    for i in range(vocab_size):\n",
        "        doc_vec[i] = g[i] * np.log(doc_vec[i] + 1.0)\n",
        "\n",
        "    residuals.append(prod_svd.residual(doc_vec))\n",
        "    prod_logger.log(stemmed, doc_vec)  # update residual only, not SVD\n",
        "\n",
        "print(f\"\\nresiduals: {residuals}\\n\")\n",
        "\n",
        "# if we trained with production data\n",
        "# svd_write_me = prod_logger.get_svd_state()\n",
        "\n",
        "# send to whylabs, no SVD state\n",
        "send_me = prod_logger.get_profile()\n",
        "\n",
        "# get stats on doc length, term length, SVD \"fit\"\n",
        "view_me = prod_svd.to_summary_dict(SummaryConfig())\n",
        "print(view_me)\n"
      ],
      "metadata": {
        "id": "DrwUYzRufU_y",
        "outputId": "8cf761e6-bb7e-4b85-f70e-92538b62557c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "residuals: [0.9374349405545456, 0.9900965672717289, 0.9113568618849668, 0.9229127995333184, 0.9026913759048323, 0.9309452409506167, 0.9600856215484892, 0.7978363639300197, 0.2485423329154347, 0.8388866693394962, 0.9219593968094596, 0.9338229346804906, 0.39892206399674773, 0.05057287344894667, 0.2561104893885517, 0.9119026566621015, 0.7955023103731856, 0.8373906262694235, 0.4977861456818984, 0.9732114162116675, 0.9374763643180631, 0.9393504075942269, 0.8652658479179123, 0.8306574119037258, 0.9100508803781197, 0.09127160761155154, 0.9118942761089684, 0.43885793955621716, 0.8920923233150638, 0.9621954071529196, 0.06675439215580092, 0.9508982030878171, 0.9417248038715189, 0.22717416450227484, 0.7054859897545632, 0.7922191445402398, 0.9109914947994936, 0.9268860122888748, 0.9512663005854589, 0.969489084485672, 0.8903536588967198, 0.9005175921107347, 0.9308675277059155, 0.9378327473580307, 0.9495842047110129, 0.9135955673015228, 0.9240933916958679, 0.9444879200285949, 0.8956638416552153, 0.8206293083437272, 0.8901878468116274, 0.9108154488867151, 0.8788866634145243, 0.9164546227988112, 0.909777833232305, 0.8504328204272483, 0.8661508003975757, 0.9191609427195151, 0.8017313904209928]\n",
            "\n",
            "{'k': 10, 'decay': 1.0, 'U': array([[-1.04566206e-02,  8.78338634e-04, -1.04260092e-02, ...,\n",
            "        -2.64379915e-02,  1.90585283e-02,  1.89877131e-02],\n",
            "       [ 3.80848718e-04,  4.22327392e-04, -4.55940239e-04, ...,\n",
            "        -5.43278775e-03,  5.76264672e-03,  1.90480104e-03],\n",
            "       [-1.03869688e-03, -2.34731186e-03, -8.86827695e-04, ...,\n",
            "         3.97455505e-05, -2.34855827e-03,  3.69291129e-03],\n",
            "       ...,\n",
            "       [ 3.36298416e-02, -1.10629720e-02, -6.61716778e-03, ...,\n",
            "         8.29192281e-03,  5.20370860e-03,  1.30220699e-02],\n",
            "       [ 7.14707516e-03,  4.67779385e-04, -5.16221792e-03, ...,\n",
            "        -3.84443830e-04, -8.86540645e-04,  2.57238677e-03],\n",
            "       [ 3.34474255e-02,  2.73304904e-02, -1.36509556e-02, ...,\n",
            "        -1.13735032e-02, -2.75588898e-02,  2.09566308e-02]]), 'S': array([10.81892584, 10.98804586, 11.06133781, 11.78461862, 12.07316165,\n",
            "       12.81755111, 13.68301058, 14.96809277, 16.10164351, 32.37152941])}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('v1.x')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f76ec28949fecf16b926a3fc5a03c1aa6468ee82fa5da4ce6fd607df021af5b5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}